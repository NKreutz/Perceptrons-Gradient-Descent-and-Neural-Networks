{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Cheat Sheet\n",
    "Turning the calculus into code for a Neural Network with 1 Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Calculate the Forward Pass\n",
    "\n",
    "## Step 1: Calculate the Linear Combination for the Inputs and Weights Layer #1\n",
    "\n",
    "### Linear Combination Equation:\n",
    "\n",
    "$$\\large \\Sigma^m_1(w_i*x_i)+b=h_{(1)}$$\n",
    "\n",
    " $m=\\,total\\,number\\,of\\,inputs$\n",
    " \n",
    " $w =\\,weight\\,layer\\,1$\n",
    " \n",
    " $x =inputs\\,to\\,neural\\,network$\n",
    " \n",
    " $b=bias$ (Optional)\n",
    " \n",
    " $\\Sigma=sum$\n",
    " \n",
    " $i=iteration$\n",
    " \n",
    "Therefore $\\Sigma^m_1(w_i*x_i)=the\\,sum\\,of\\,the\\,product\\,of\\,weight\\,layer\\,one * input\\,to\\,the\\,neural\\,network\\,for\\,each\\,iteration\\,in\\,range\\,1\\,to\\,the\\,total\\,number\\,of\\,inputs$\n",
    "\n",
    "$h_{(1)}=\\,input\\,to\\,hidden\\,layer\\,1$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## In code, looks like this:\n",
    "import numpy as np\n",
    "input_to_hidden_layer_1 = np.dot(weight_layer_one,inputs_to_neural_network) + bias #np.dot() = numpy dot product function\n",
    "# or:\n",
    "h_one = np.dot(w,x) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the Activation Function for the Hidden Layer (aka Activation Function Node, Perceptron or neuron) and Calculate the Output \n",
    "\n",
    "Common Types of Activation Functions: Heaviside Step, Logistic (Sigmoid), tanh and softmax\n",
    "\n",
    "### Logistic (Sigmoid) Function\n",
    "\n",
    "$$\\large f(h_{(1)})=sigmoid(h_{(1)})=\\frac{1}{(1+e^{-h_{(1)}})}= k_{(1)}$$\n",
    "\n",
    "$f=activation\\,function(sigmoid)$\n",
    "\n",
    "$h_{(1)}=\\,input\\,to\\,hidden\\,layer\\,one$\n",
    "\n",
    "$e=\\,Euler's\\,Number\\,(an\\,irrational\\,number,\\,approximately\\,2.718281)$\n",
    "\n",
    "$k_{(1)}=output\\,of\\,hidden\\,layer\\,one$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## In code, looks like this:\n",
    "import numpy as np\n",
    "def sigmoid(input_to_hidden_layer_one):\n",
    "    return 1 / (1 + np.exp(-input_to_hidden_layer_one)) # np.exp = numpy exponential function\n",
    "output_of_hidden_layer_one = sigmoid(input_to_hidden_layer_one)\n",
    "# or:\n",
    "def sigmoid(h_one):\n",
    "    return 1 / (1 + np.exp(-h_one)) \n",
    "k_one = sigmoid(h_one)\n",
    "# Commonly, x is used as the parameter in the definition of the sigmoid function\n",
    "# however, I used h as this function is used on the input h(n), not on the initial inputs (x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate the Linear Combination for the Output of Hidden Layer #1 and Weights Layer #2\n",
    "\n",
    "For each new layer, the output of the previous hidden layer becomes the input to the current hidden layer, and there is no longer a bias factor:\n",
    "\n",
    "Therefore the Linear Combination Equation becomes:\n",
    "\n",
    "$$\\large \\Sigma^{m}_1(W_i*k_{(1)i})=h_{(2)}$$\n",
    "\n",
    "$m=number\\,of\\,nodes\\,in\\,hidden\\,layer\\,one$\n",
    "\n",
    "$i=iteration$\n",
    "\n",
    "$W= weight\\,layer\\,2$\n",
    "\n",
    "$k_{(1)}= output\\,of\\,hidden\\,layer\\,one$\n",
    "\n",
    "Therefore: $\\Sigma^p_1(W_i*k_{(1)i})=the\\,sum\\,of\\,the\\,product\\,of\\,weight\\,layer\\,2 * the\\,output\\,of\\,hidden\\,layer\\,one\\,for\\,each\\,iteration\\,in\\,range\\,1\\,to\\,the\\,total\\,number\\,of\\,nodes\\,in\\,hidden\\,layer\\,one$\n",
    "\n",
    "$h_{(2)}= input\\,to\\,ouput\\,layer$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In code, looks like this:\n",
    "input_to_output_layer = np.dot(weight_layer_two, output_of_hidden_layer_one)\n",
    "#or:\n",
    "h_two = np.dot(W,k_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Calculate the Activation Function for the Output Layer\n",
    "\n",
    "\n",
    "$$\\large f(h_{(2)})=sigmoid(h_{(2)})=\\frac{1}{(1+e^{-h_{(2)}})}= \\hat y$$\n",
    "\n",
    "$\\hat y = output\\,of\\,the\\,neural\\,network = prediction$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In code, looks like this:\n",
    "output_of_neural_network = sigmoid(input_to_output_layer)\n",
    "#or:\n",
    "prediction = sigmoid(h_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Calculate the Backwards Pass\n",
    "\n",
    "## Step 1: Calculate the Error Gradient for Weight Layer #2\n",
    "\n",
    "$$\\large \\delta_{(W)}=(y-\\hat y)* \\hat y * (1 - \\hat y)$$\n",
    "\n",
    "$\\delta_{(W)} = Error\\,gradient\\,for\\,weight\\,layer\\,2$\n",
    "\n",
    "$y = target\\,values$\n",
    "\n",
    "$\\hat y = output\\,of\\,the\\,neural\\,network = prediction$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In code, looks like:\n",
    "Error_gradient_weight_layer_two = (target_values - output_of_neural_network) * output_of_neural_network * (1 - output_of_neural_network)\n",
    "# or:\n",
    "Err_grad_W = (y - prediction) * prediction * (1 - prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculate Error Gradient for Weight Layer #1\n",
    "\n",
    "$$\\large \\delta_{(w)}=W_i*\\delta_{(W)} * k_{(1)} * (1-k_{(1)})$$\n",
    "\n",
    "$\\delta_{(w)} = Error\\,gradient\\,for\\,weight\\,layer\\,1$\n",
    "\n",
    "$W= weight\\,layer\\,2$\n",
    "\n",
    "$\\delta_{(W)} = Error\\,gradient\\,for\\,weight\\,layer\\,2$\n",
    "\n",
    "$k_{(1)}= output\\,of\\,hidden\\,layer\\,one$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In code, looks like:\n",
    "Error_gradient_weight_layer_one = weight_layer_two * Error_gradient_weight_layer_two * output_of_hidden_layer_one * (1 - output_of_hidden_layer_one)\n",
    "# or:\n",
    "Err_grad_w = W * Err_grad_W * k_one * (1 - k_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate the Weight Change Step for Weight Layer #2\n",
    "\n",
    "$$\\large \\Delta W = \\eta *\\delta_{(W)}*k_{(1)}$$\n",
    "\n",
    "$\\Delta W = weight\\,change\\,step\\,for\\,weight\\,layer\\,2$\n",
    "\n",
    "$\\eta = learning\\,rate$\n",
    "\n",
    "$\\delta_{(W)} = Error\\,gradient\\,for\\,weight\\,layer\\,2$\n",
    "\n",
    "$k_{(1)} = output\\,of\\,hidden\\,layer\\,one$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In code, looks like:\n",
    "weight_change_step_weight_layer_two = learning_rate * Error_gradient_weight_layer_two * output_of_hidden_layer_one\n",
    "# or:\n",
    "del_W = learnrate * Err_grad_W * k_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Calculate the Weight Change Step for Weight Layer #1\n",
    "\n",
    "$$\\large \\Delta w = \\eta *\\delta _{(w)} *x$$\n",
    "\n",
    "$\\Delta w = weight\\,change\\,step\\,for\\,weight\\,layer\\,1$\n",
    "\n",
    "$\\eta = learning\\,rate$\n",
    "\n",
    "$\\delta_{(w)} = Error\\,gradient\\,for\\,weight\\,layer\\,1$\n",
    "\n",
    "$x = inputs\\,to\\,neural\\,network$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In code, looks like:\n",
    "weight_change_step_weight_layer_one = learning_rate * Error_gradient_weight_layer_one * inputs_to_neural_network\n",
    "# or:\n",
    "del_w = learnrate * Err_grad_w * x\n",
    "# If this throws back a Value Error, add [:,None] to the end of the line to clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Update Weights for Weight Layer #1\n",
    "\n",
    "$$\\large w = w + \\frac {\\eta \\Delta w}{r}$$\n",
    "\n",
    "$The\\,first\\,w = Updated\\,weight\\,layer\\,1$\n",
    "\n",
    "$The\\,second\\,w = Current\\,weight\\,layer\\,1$\n",
    "\n",
    "$\\eta = learning\\,rate$\n",
    "\n",
    "$\\Delta w = weight\\,change\\,step\\,for\\,weight\\,layer\\,1$\n",
    "\n",
    "$r = number\\,of\\,records$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In code, looks like:\n",
    "weight_layer_one += (learning_rate * weight_change_step_weight_layer_one)/number_of_records\n",
    "# or:\n",
    "w += learnrate * del_w / r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Update Weights for Weight Layer #2\n",
    "\n",
    "$$\\large W = W + \\frac {\\eta \\Delta W}{r}$$\n",
    "\n",
    "$The\\,first\\,W = Updated\\,weight\\,layer\\,2$\n",
    "\n",
    "$The\\,second\\,W = Current\\,weight\\,layer\\,2$\n",
    "\n",
    "$\\eta = learning\\,rate$\n",
    "\n",
    "$\\Delta w = weight\\,change\\,step\\,for\\,weight\\,layer\\,2$\n",
    "\n",
    "$r = number\\,of\\,records$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In code, looks like:\n",
    "weight_layer_two += (learning_rate * weight_change_step_weight_layer_two)/number_of_records\n",
    "# or:\n",
    "W += learnrate * del_W / r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import your Packages:\n",
    "import numpy as np\n",
    "\n",
    "# Define your Activation Function:\n",
    "def sigmoid(input_to_hidden_layer_one):\n",
    "    return 1 / (1 + np.exp(-input_to_hidden_layer_one)) # np.exp = numpy exponential function\n",
    "#or: def sigmoid(h_one):\n",
    "    #return 1 / (1 + np.exp(-h_one)) \n",
    "    \n",
    "# Part 1: Calculate the Forward Pass\n",
    "\n",
    "## Step 1: Calculate the Linear Combination for the Inputs and Weights Layer #1\n",
    "input_to_hidden_layer_1 = np.dot(weight_layer_one,inputs_to_neural_network) + bias #np.dot() = numpy dot product function\n",
    "# or: h_one = np.dot(w,x) + b\n",
    "\n",
    "## Step 2: Calculate the Activation Function for the Hidden Layer\n",
    "output_of_hidden_layer_one = sigmoid(input_to_hidden_layer_one)\n",
    "# or: k_one = sigmoid(h_one)\n",
    "\n",
    "## Step 3: Calculate the Linear Combination for the Output of Hidden Layer #1 and Weights Layer #2\n",
    "input_to_output_layer = np.dot(weight_layer_two, output_of_hidden_layer_one)\n",
    "#or: h_two = np.dot(W,k_one)\n",
    "\n",
    "## Step 4: Calculate the Activation Function for the Output Layer\n",
    "output_of_neural_network = sigmoid(input_to_output_layer)\n",
    "#or: prediction = sigmoid(h_n)\n",
    "\n",
    "# Part 2: Calculate the Backwards Pass\n",
    "\n",
    "## Step 1: Calculate the Error Gradient for Weight Layer #2\n",
    "Error_gradient_weight_layer_two = (target_values - output_of_neural_network) * output_of_neural_network * (1 - output_of_neural_network)\n",
    "# or: Err_grad_W = (y - prediction) * prediction * (1 - prediction)\n",
    "\n",
    "## Step 2: Calculate Error Gradient for Weight Layer #1\n",
    "Error_gradient_weight_layer_one = weight_layer_two * Error_gradient_weight_layer_two * output_of_hidden_layer_one * (1 - output_of_hidden_layer_one)\n",
    "# or: Err_grad_w = W * Err_grad_W * k_one * (1 - k_one)\n",
    "\n",
    "## Step 3: Calculate the Weight Change Step for Weight Layer #2\n",
    "weight_change_step_weight_layer_two = learning_rate * Error_gradient_weight_layer_two * output_of_hidden_layer_one\n",
    "# or: del_W = learnrate * Err_grad_W * k_one\n",
    "\n",
    "## Step 4: Calculate the Weight Change Step for Weight Layer #1\n",
    "weight_change_step_weight_layer_one = learning_rate * Error_gradient_weight_layer_one * inputs_to_neural_network\n",
    "# or: del_w = learnrate * Err_grad_w * x\n",
    "# If this throws back a Value Error, add [:,None] to the end of the line to clear\n",
    "\n",
    "## Step 5: Update Weights for Weight Layer #1\n",
    "weight_layer_one += (learning_rate * weight_change_step_weight_layer_one)/number_of_records\n",
    "# or: w += learnrate * del_w / r\n",
    "\n",
    "## Step 6: Update Weights for Weight Layer #2\n",
    "weight_layer_two += (learning_rate * weight_change_step_weight_layer_two)/number_of_records\n",
    "# or: W += learnrate * del_W / r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
